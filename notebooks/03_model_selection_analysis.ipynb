{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521ce5ad",
   "metadata": {},
   "source": [
    "# MET Prediction Model Selection and Analysis\n",
    "\n",
    "This notebook addresses the key questions for ADAMMA Challenge 2025:\n",
    "1. **Time Window Optimization**: Analyzing whether momentary acceleration spikes should trigger immediate class changes or if we need smoothing\n",
    "2. **Model Comparison**: Comparing multiple ML models for MET class prediction\n",
    "3. **Mobile Deployment**: Preparing the best model for iOS app integration\n",
    "\n",
    "## Key Research Questions:\n",
    "- Should momentary acceleration spikes trigger activity class changes?\n",
    "- What's the optimal time window for prediction?\n",
    "- Which ML model provides the best accuracy-performance trade-off for mobile deployment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6737ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from scipy import stats\n",
    "from scipy.signal import butter, filtfilt\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project paths\n",
    "sys.path.append('../src')\n",
    "from data_processing.wisdm_processor import WISDMDataProcessor\n",
    "from feature_extraction.feature_extractor import METFeatureExtractor\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326763a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Prepare Dataset\n",
    "print(\"Loading WISDM dataset...\")\n",
    "\n",
    "# Initialize data processor\n",
    "processor = WISDMDataProcessor(data_dir=\"../data\")\n",
    "\n",
    "# Load processed data or create synthetic if needed\n",
    "try:\n",
    "    df = pd.read_csv(\"../data/processed/synthetic_met_data.csv\")\n",
    "    print(f\"Loaded existing dataset: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Creating new dataset...\")\n",
    "    df = processor.load_and_process_data()\n",
    "\n",
    "# Display dataset info\n",
    "print(f\"\\nDataset Overview:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nMET class distribution:\")\n",
    "print(df['met_class'].value_counts().sort_index())\n",
    "print(f\"\\nActivity distribution:\")\n",
    "print(df['activity'].value_counts())\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing and Basic Feature Engineering\n",
    "def calculate_magnitude(x, y, z):\n",
    "    \"\"\"Calculate acceleration magnitude\"\"\"\n",
    "    return np.sqrt(x**2 + y**2 + z**2)\n",
    "\n",
    "def calculate_jerk(acc_data):\n",
    "    \"\"\"Calculate jerk (rate of change of acceleration)\"\"\"\n",
    "    return np.diff(acc_data, prepend=acc_data[0])\n",
    "\n",
    "def apply_smoothing_filter(data, cutoff_freq=5, fs=20, order=4):\n",
    "    \"\"\"Apply low-pass Butterworth filter to smooth data\"\"\"\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff_freq / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "# Calculate basic features\n",
    "df['magnitude'] = calculate_magnitude(df['x'], df['y'], df['z'])\n",
    "df['magnitude_smoothed'] = apply_smoothing_filter(df['magnitude'].values)\n",
    "\n",
    "# Calculate statistical features per user-activity session\n",
    "print(\"Calculating advanced features...\")\n",
    "\n",
    "def extract_features_for_window(group, window_size=50):\n",
    "    \"\"\"Extract features for a sliding window\"\"\"\n",
    "    features = []\n",
    "    for i in range(0, len(group) - window_size + 1, window_size//2):  # 50% overlap\n",
    "        window = group.iloc[i:i+window_size]\n",
    "        \n",
    "        feature_row = {\n",
    "            'user': window['user'].iloc[0],\n",
    "            'activity': window['activity'].iloc[0],\n",
    "            'met_class': window['met_class'].iloc[0],\n",
    "            \n",
    "            # Magnitude features\n",
    "            'mag_mean': window['magnitude'].mean(),\n",
    "            'mag_std': window['magnitude'].std(),\n",
    "            'mag_min': window['magnitude'].min(),\n",
    "            'mag_max': window['magnitude'].max(),\n",
    "            'mag_range': window['magnitude'].max() - window['magnitude'].min(),\n",
    "            'mag_iqr': window['magnitude'].quantile(0.75) - window['magnitude'].quantile(0.25),\n",
    "            \n",
    "            # Smoothed magnitude features  \n",
    "            'mag_smooth_mean': window['magnitude_smoothed'].mean(),\n",
    "            'mag_smooth_std': window['magnitude_smoothed'].std(),\n",
    "            \n",
    "            # Individual axis features\n",
    "            'x_mean': window['x'].mean(),\n",
    "            'x_std': window['x'].std(),\n",
    "            'y_mean': window['y'].mean(), \n",
    "            'y_std': window['y'].std(),\n",
    "            'z_mean': window['z'].mean(),\n",
    "            'z_std': window['z'].std(),\n",
    "            \n",
    "            # Cross-axis correlations\n",
    "            'xy_corr': window['x'].corr(window['y']),\n",
    "            'xz_corr': window['x'].corr(window['z']),\n",
    "            'yz_corr': window['y'].corr(window['z']),\n",
    "            \n",
    "            # Zero crossing rates\n",
    "            'x_zcr': np.sum(np.diff(np.signbit(window['x']))) / len(window),\n",
    "            'y_zcr': np.sum(np.diff(np.signbit(window['y']))) / len(window),\n",
    "            'z_zcr': np.sum(np.diff(np.signbit(window['z']))) / len(window),\n",
    "        }\n",
    "        features.append(feature_row)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features for different window sizes\n",
    "window_sizes = [25, 50, 100]  # Corresponding to ~1.25s, 2.5s, 5s at 20Hz\n",
    "feature_datasets = {}\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    print(f\"Extracting features for window size: {window_size}\")\n",
    "    all_features = []\n",
    "    \n",
    "    for (user, activity), group in df.groupby(['user', 'activity']):\n",
    "        if len(group) >= window_size:\n",
    "            features = extract_features_for_window(group, window_size)\n",
    "            all_features.extend(features)\n",
    "    \n",
    "    feature_df = pd.DataFrame(all_features)\n",
    "    feature_df = feature_df.dropna()  # Remove rows with NaN correlations\n",
    "    feature_datasets[window_size] = feature_df\n",
    "    \n",
    "    print(f\"  - Generated {len(feature_df)} feature windows\")\n",
    "    print(f\"  - MET class distribution: {feature_df['met_class'].value_counts().to_dict()}\")\n",
    "\n",
    "print(\"Feature extraction complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d60537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Window Analysis and Threshold Investigation\n",
    "print(\"=== TIME WINDOW ANALYSIS ===\")\n",
    "\n",
    "# Analyze the effect of different window sizes on classification stability\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Magnitude variance across different window sizes\n",
    "for i, window_size in enumerate(window_sizes):\n",
    "    feature_df = feature_datasets[window_size]\n",
    "    \n",
    "    # Plot magnitude statistics by MET class\n",
    "    ax = axes[i//2, i%2] if len(window_sizes) > 2 else axes[i]\n",
    "    \n",
    "    for met_class in sorted(feature_df['met_class'].unique()):\n",
    "        class_data = feature_df[feature_df['met_class'] == met_class]\n",
    "        ax.scatter(class_data['mag_mean'], class_data['mag_std'], \n",
    "                  label=f'MET Class {met_class}', alpha=0.6)\n",
    "    \n",
    "    ax.set_xlabel('Mean Magnitude')\n",
    "    ax.set_ylabel('Std Magnitude') \n",
    "    ax.set_title(f'Window Size: {window_size} samples')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "# Plot comparison of smoothed vs raw magnitude sensitivity\n",
    "if len(window_sizes) > 2:\n",
    "    ax = axes[1, 1]\n",
    "else:\n",
    "    fig2, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "# Sample data for demonstration\n",
    "sample_data = df[df['user'] == 1].head(200)\n",
    "ax.plot(sample_data['magnitude'], label='Raw Magnitude', alpha=0.7)\n",
    "ax.plot(sample_data['magnitude_smoothed'], label='Smoothed Magnitude', alpha=0.7)\n",
    "ax.axhline(y=1.05, color='r', linestyle='--', label='Threshold (Heuristic)')\n",
    "ax.set_xlabel('Time (samples)')\n",
    "ax.set_ylabel('Acceleration Magnitude')\n",
    "    ax.set_title('Raw vs Smoothed Magnitude - Reducing Noise from Sudden Movements')\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate threshold crossing analysis\n",
    "print(\"\\n=== THRESHOLD CROSSING ANALYSIS ===\")\n",
    "threshold = 1.05  # Current heuristic threshold\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    feature_df = feature_datasets[window_size]\n",
    "    \n",
    "    # Calculate how often magnitude crosses threshold vs sustained activity\n",
    "    raw_crossings = (feature_df['mag_max'] > threshold).sum()\n",
    "    sustained_crossings = (feature_df['mag_mean'] > threshold).sum()\n",
    "    \n",
    "    print(f\"Window Size {window_size}:\")\n",
    "    print(f\"  - Raw magnitude crossings: {raw_crossings}/{len(feature_df)} ({raw_crossings/len(feature_df)*100:.1f}%)\")\n",
    "    print(f\"  - Sustained (mean) crossings: {sustained_crossings}/{len(feature_df)} ({sustained_crossings/len(feature_df)*100:.1f}%)\")\n",
    "    print(f\"  - Reduction factor: {raw_crossings/max(sustained_crossings, 1):.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847b8ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison and Selection\n",
    "print(\"=== MODEL COMPARISON ===\")\n",
    "\n",
    "# Define models to compare\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    'SVM': SVC(random_state=42, probability=True),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42, n_estimators=100),\n",
    "    'Neural Network': MLPClassifier(random_state=42, max_iter=500)\n",
    "}\n",
    "\n",
    "# Prepare feature columns (exclude metadata)\n",
    "feature_cols = ['mag_mean', 'mag_std', 'mag_min', 'mag_max', 'mag_range', 'mag_iqr',\n",
    "                'mag_smooth_mean', 'mag_smooth_std',\n",
    "                'x_mean', 'x_std', 'y_mean', 'y_std', 'z_mean', 'z_std',\n",
    "                'xy_corr', 'xz_corr', 'yz_corr', 'x_zcr', 'y_zcr', 'z_zcr']\n",
    "\n",
    "# Test each window size and model combination\n",
    "results = {}\n",
    "\n",
    "for window_size in window_sizes:\n",
    "    print(f\"\\n--- Window Size: {window_size} samples ---\")\n",
    "    feature_df = feature_datasets[window_size]\n",
    "    \n",
    "    X = feature_df[feature_cols]\n",
    "    y = feature_df['met_class']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                        random_state=42, stratify=y)\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    window_results = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"  Training {model_name}...\")\n",
    "        \n",
    "        # Train model\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Test inference time (important for mobile)\n",
    "        start_time = time.time()\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        inference_time = (time.time() - start_time) / len(X_test) * 1000  # ms per prediction\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "        \n",
    "        window_results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'cv_mean': cv_scores.mean(),\n",
    "            'cv_std': cv_scores.std(),\n",
    "            'training_time': training_time,\n",
    "            'inference_time_ms': inference_time,\n",
    "            'model': model,\n",
    "            'scaler': scaler,\n",
    "            'y_pred': y_pred,\n",
    "            'y_test': y_test\n",
    "        }\n",
    "        \n",
    "        print(f\"    Accuracy: {accuracy:.4f} ± {cv_scores.std():.4f}\")\n",
    "        print(f\"    Inference: {inference_time:.2f}ms per prediction\")\n",
    "    \n",
    "    results[window_size] = window_results\n",
    "\n",
    "print(\"\\n=== SUMMARY RESULTS ===\")\n",
    "for window_size in window_sizes:\n",
    "    print(f\"\\nWindow Size {window_size}:\")\n",
    "    for model_name in models.keys():\n",
    "        res = results[window_size][model_name]\n",
    "        print(f\"  {model_name}: {res['accuracy']:.4f} (±{res['cv_std']:.4f}) - {res['inference_time_ms']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4385c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning for Best Models\n",
    "print(\"=== HYPERPARAMETER TUNING ===\")\n",
    "\n",
    "# Identify best performing models across all window sizes\n",
    "best_combinations = []\n",
    "for window_size in window_sizes:\n",
    "    for model_name in models.keys():\n",
    "        result = results[window_size][model_name]\n",
    "        best_combinations.append({\n",
    "            'window_size': window_size,\n",
    "            'model_name': model_name,\n",
    "            'accuracy': result['accuracy'],\n",
    "            'inference_time': result['inference_time_ms']\n",
    "        })\n",
    "\n",
    "# Sort by accuracy and filter by reasonable inference time (<50ms)\n",
    "best_combinations = sorted(best_combinations, key=lambda x: x['accuracy'], reverse=True)\n",
    "mobile_suitable = [x for x in best_combinations if x['inference_time'] < 50]\n",
    "\n",
    "print(\"Top 3 mobile-suitable model combinations:\")\n",
    "for i, combo in enumerate(mobile_suitable[:3]):\n",
    "    print(f\"{i+1}. {combo['model_name']} (Window: {combo['window_size']}) - \"\n",
    "          f\"Acc: {combo['accuracy']:.4f}, Time: {combo['inference_time']:.1f}ms\")\n",
    "\n",
    "# Tune the best model\n",
    "best_combo = mobile_suitable[0]\n",
    "best_window = best_combo['window_size']\n",
    "best_model_name = best_combo['model_name']\n",
    "\n",
    "print(f\"\\nTuning {best_model_name} with window size {best_window}...\")\n",
    "\n",
    "# Get data for best configuration\n",
    "feature_df = feature_datasets[best_window]\n",
    "X = feature_df[feature_cols]\n",
    "y = feature_df['met_class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define parameter grids for tuning\n",
    "param_grids = {\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.1, 0.05, 0.01]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "}\n",
    "\n",
    "if best_model_name in param_grids:\n",
    "    param_grid = param_grids[best_model_name]\n",
    "    base_model = models[best_model_name]\n",
    "    \n",
    "    # Use TimeSeriesSplit for more realistic evaluation\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    \n",
    "    grid_search = GridSearchCV(base_model, param_grid, cv=tscv, \n",
    "                              scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    \n",
    "    print(\"Running grid search...\")\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Get best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    \n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Test tuned model\n",
    "    y_pred_tuned = best_model.predict(X_test_scaled)\n",
    "    tuned_accuracy = accuracy_score(y_test, y_pred_tuned)\n",
    "    \n",
    "    print(f\"Test accuracy (tuned): {tuned_accuracy:.4f}\")\n",
    "    print(f\"Improvement: {tuned_accuracy - best_combo['accuracy']:.4f}\")\n",
    "else:\n",
    "    best_model = results[best_window][best_model_name]['model']\n",
    "    best_params = \"Default parameters\"\n",
    "    y_pred_tuned = results[best_window][best_model_name]['y_pred']\n",
    "    tuned_accuracy = results[best_window][best_model_name]['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d68c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation and Performance Analysis\n",
    "print(\"=== DETAILED MODEL EVALUATION ===\")\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tuned, \n",
    "                          target_names=['Sedentary', 'Light', 'Moderate', 'Vigorous']))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_tuned)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Confusion matrix heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Sedentary', 'Light', 'Moderate', 'Vigorous'],\n",
    "            yticklabels=['Sedentary', 'Light', 'Moderate', 'Vigorous'],\n",
    "            ax=axes[0])\n",
    "axes[0].set_title(f'Confusion Matrix - {best_model_name}')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Model comparison chart\n",
    "comparison_data = []\n",
    "for window_size in window_sizes:\n",
    "    for model_name in models.keys():\n",
    "        result = results[window_size][model_name]\n",
    "        comparison_data.append({\n",
    "            'Model': model_name,\n",
    "            'Window Size': window_size,\n",
    "            'Accuracy': result['accuracy'],\n",
    "            'Inference Time (ms)': result['inference_time_ms']\n",
    "        })\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Accuracy vs Inference Time scatter plot\n",
    "for model_name in models.keys():\n",
    "    model_data = comp_df[comp_df['Model'] == model_name]\n",
    "    axes[1].scatter(model_data['Inference Time (ms)'], model_data['Accuracy'], \n",
    "                   label=model_name, s=100, alpha=0.7)\n",
    "\n",
    "axes[1].set_xlabel('Inference Time (ms)')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Model Performance: Accuracy vs Inference Time')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "axes[1].axvline(x=50, color='r', linestyle='--', alpha=0.5, label='Mobile Limit (50ms)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy analysis\n",
    "class_accuracies = []\n",
    "for i, class_name in enumerate(['Sedentary', 'Light', 'Moderate', 'Vigorous']):\n",
    "    class_mask = (y_test == i)\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = accuracy_score(y_test[class_mask], y_pred_tuned[class_mask])\n",
    "        class_accuracies.append(class_acc)\n",
    "        print(f\"{class_name} accuracy: {class_acc:.4f}\")\n",
    "\n",
    "# Plot class-wise accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(['Sedentary', 'Light', 'Moderate', 'Vigorous'], class_accuracies, \n",
    "        color=['blue', 'green', 'orange', 'red'], alpha=0.7)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Per-Class Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "for i, acc in enumerate(class_accuracies):\n",
    "    plt.text(i, acc + 0.02, f'{acc:.3f}', ha='center')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73708bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "print(\"=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Tree-based model feature importance\n",
    "    importances = best_model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance_df.head(15)\n",
    "    plt.barh(range(len(top_features)), top_features['importance'], \n",
    "             color='steelblue', alpha=0.7)\n",
    "    plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(f'Top 15 Feature Importances - {best_model_name}')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 10 most important features:\")\n",
    "    for i, row in feature_importance_df.head(10).iterrows():\n",
    "        print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "elif hasattr(best_model, 'coef_'):\n",
    "    # Linear model coefficients\n",
    "    coefs = np.abs(best_model.coef_).mean(axis=0)  # Average across classes\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': coefs\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 features by coefficient magnitude:\")\n",
    "    for i, row in feature_importance_df.head(10).iterrows():\n",
    "        print(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Correlation analysis between features and MET classes\n",
    "print(\"\\n=== FEATURE-TARGET CORRELATION ===\")\n",
    "feature_df = feature_datasets[best_window]\n",
    "correlations = {}\n",
    "\n",
    "for feature in feature_cols:\n",
    "    if feature in feature_df.columns:\n",
    "        corr = feature_df[feature].corr(feature_df['met_class'])\n",
    "        if not np.isnan(corr):\n",
    "            correlations[feature] = abs(corr)\n",
    "\n",
    "# Sort by correlation strength\n",
    "sorted_correlations = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 10 features by correlation with MET class:\")\n",
    "for feature, corr in sorted_correlations[:10]:\n",
    "    print(f\"{feature}: {corr:.4f}\")\n",
    "\n",
    "# Create correlation heatmap for top features\n",
    "top_corr_features = [item[0] for item in sorted_correlations[:10]]\n",
    "corr_data = feature_df[top_corr_features + ['met_class']].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_data, annot=True, cmap='coolwarm', center=0, \n",
    "            fmt='.3f', square=True)\n",
    "plt.title('Feature Correlation Matrix (Top 10 Features)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae01d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time Prediction Pipeline Implementation  \n",
    "print(\"=== REAL-TIME PREDICTION PIPELINE ===\")\n",
    "\n",
    "class METRealTimePipeline:\n",
    "    \"\"\"Complete pipeline for real-time MET prediction\"\"\"\n",
    "    \n",
    "    def __init__(self, model, scaler, feature_cols, window_size=50):\n",
    "        self.model = model\n",
    "        self.scaler = scaler\n",
    "        self.feature_cols = feature_cols\n",
    "        self.window_size = window_size\n",
    "        self.data_buffer = []\n",
    "        self.prediction_history = []\n",
    "        \n",
    "    def add_accelerometer_reading(self, x, y, z, timestamp=None):\n",
    "        \"\"\"Add new accelerometer reading to buffer\"\"\"\n",
    "        magnitude = np.sqrt(x**2 + y**2 + z**2)\n",
    "        self.data_buffer.append({\n",
    "            'x': x, 'y': y, 'z': z, \n",
    "            'magnitude': magnitude,\n",
    "            'timestamp': timestamp or time.time()\n",
    "        })\n",
    "        \n",
    "        # Keep only the required window size\n",
    "        if len(self.data_buffer) > self.window_size * 2:\n",
    "            self.data_buffer = self.data_buffer[-self.window_size * 2:]\n",
    "    \n",
    "    def extract_features_from_buffer(self):\n",
    "        \"\"\"Extract features from current buffer\"\"\"\n",
    "        if len(self.data_buffer) < self.window_size:\n",
    "            return None\n",
    "        \n",
    "        # Use the most recent window\n",
    "        window_data = self.data_buffer[-self.window_size:]\n",
    "        df_window = pd.DataFrame(window_data)\n",
    "        \n",
    "        # Apply smoothing\n",
    "        df_window['magnitude_smoothed'] = apply_smoothing_filter(df_window['magnitude'].values)\n",
    "        \n",
    "        # Extract features (same as training)\n",
    "        features = {\n",
    "            'mag_mean': df_window['magnitude'].mean(),\n",
    "            'mag_std': df_window['magnitude'].std(),\n",
    "            'mag_min': df_window['magnitude'].min(),\n",
    "            'mag_max': df_window['magnitude'].max(),\n",
    "            'mag_range': df_window['magnitude'].max() - df_window['magnitude'].min(),\n",
    "            'mag_iqr': df_window['magnitude'].quantile(0.75) - df_window['magnitude'].quantile(0.25),\n",
    "            'mag_smooth_mean': df_window['magnitude_smoothed'].mean(),\n",
    "            'mag_smooth_std': df_window['magnitude_smoothed'].std(),\n",
    "            'x_mean': df_window['x'].mean(),\n",
    "            'x_std': df_window['x'].std(),\n",
    "            'y_mean': df_window['y'].mean(), \n",
    "            'y_std': df_window['y'].std(),\n",
    "            'z_mean': df_window['z'].mean(),\n",
    "            'z_std': df_window['z'].std(),\n",
    "            'xy_corr': df_window['x'].corr(df_window['y']),\n",
    "            'xz_corr': df_window['x'].corr(df_window['z']),\n",
    "            'yz_corr': df_window['y'].corr(df_window['z']),\n",
    "            'x_zcr': np.sum(np.diff(np.signbit(df_window['x']))) / len(df_window),\n",
    "            'y_zcr': np.sum(np.diff(np.signbit(df_window['y']))) / len(df_window),\n",
    "            'z_zcr': np.sum(np.diff(np.signbit(df_window['z']))) / len(df_window),\n",
    "        }\n",
    "        \n",
    "        # Handle NaN correlations\n",
    "        for key in ['xy_corr', 'xz_corr', 'yz_corr']:\n",
    "            if np.isnan(features[key]):\n",
    "                features[key] = 0.0\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def predict_met_class(self):\n",
    "        \"\"\"Predict MET class from current buffer\"\"\"\n",
    "        features = self.extract_features_from_buffer()\n",
    "        if features is None:\n",
    "            return None, 0.0\n",
    "        \n",
    "        # Convert to feature array\n",
    "        feature_array = np.array([features[col] for col in self.feature_cols]).reshape(1, -1)\n",
    "        \n",
    "        # Scale and predict\n",
    "        feature_array_scaled = self.scaler.transform(feature_array)\n",
    "        prediction = self.model.predict(feature_array_scaled)[0]\n",
    "        \n",
    "        # Get confidence if available\n",
    "        if hasattr(self.model, 'predict_proba'):\n",
    "            probabilities = self.model.predict_proba(feature_array_scaled)[0]\n",
    "            confidence = probabilities.max()\n",
    "        else:\n",
    "            confidence = 1.0\n",
    "        \n",
    "        # Store prediction\n",
    "        self.prediction_history.append({\n",
    "            'timestamp': time.time(),\n",
    "            'prediction': prediction,\n",
    "            'confidence': confidence\n",
    "        })\n",
    "        \n",
    "        return prediction, confidence\n",
    "    \n",
    "    def get_smoothed_prediction(self, smoothing_window=3):\n",
    "        \"\"\"Get smoothed prediction to avoid rapid class changes\"\"\"\n",
    "        if len(self.prediction_history) < smoothing_window:\n",
    "            return self.prediction_history[-1]['prediction'] if self.prediction_history else 0\n",
    "        \n",
    "        recent_predictions = [p['prediction'] for p in self.prediction_history[-smoothing_window:]]\n",
    "        # Use mode (most common prediction)\n",
    "        return max(set(recent_predictions), key=recent_predictions.count)\n",
    "\n",
    "# Create pipeline instance\n",
    "pipeline = METRealTimePipeline(best_model, scaler, feature_cols, best_window)\n",
    "\n",
    "print(f\"Pipeline created with:\")\n",
    "print(f\"  - Model: {best_model_name}\")\n",
    "print(f\"  - Window size: {best_window} samples\")\n",
    "print(f\"  - Features: {len(feature_cols)} features\")\n",
    "\n",
    "# Test pipeline with sample data\n",
    "print(\"\\n=== PIPELINE TESTING ===\")\n",
    "test_user_data = feature_datasets[best_window][feature_datasets[best_window]['user'] == 1].head(100)\n",
    "\n",
    "print(\"Testing pipeline with sample accelerometer data...\")\n",
    "predictions = []\n",
    "confidences = []\n",
    "\n",
    "# Simulate real-time data input\n",
    "for _, row in test_user_data.iterrows():\n",
    "    # Simulate adding accelerometer readings (we need to reverse engineer from features)\n",
    "    # For testing, we'll use synthetic data with known patterns\n",
    "    if row['met_class'] == 0:  # Sedentary\n",
    "        x, y, z = np.random.normal(0, 0.5), np.random.normal(0, 0.5), np.random.normal(9.8, 0.5)\n",
    "    elif row['met_class'] == 1:  # Light\n",
    "        x, y, z = np.random.normal(0, 1.5), np.random.normal(0, 1.5), np.random.normal(9.8, 1.5)\n",
    "    elif row['met_class'] == 2:  # Moderate  \n",
    "        x, y, z = np.random.normal(0, 3.0), np.random.normal(0, 3.0), np.random.normal(9.8, 3.0)\n",
    "    else:  # Vigorous\n",
    "        x, y, z = np.random.normal(0, 5.0), np.random.normal(0, 5.0), np.random.normal(9.8, 5.0)\n",
    "    \n",
    "    pipeline.add_accelerometer_reading(x, y, z)\n",
    "    \n",
    "    # Try to get prediction\n",
    "    pred, conf = pipeline.predict_met_class()\n",
    "    if pred is not None:\n",
    "        predictions.append(pred)\n",
    "        confidences.append(conf)\n",
    "\n",
    "if predictions:\n",
    "    print(f\"Pipeline test completed:\")\n",
    "    print(f\"  - Made {len(predictions)} predictions\")\n",
    "    print(f\"  - Average confidence: {np.mean(confidences):.3f}\")\n",
    "    print(f\"  - Prediction distribution: {np.bincount(predictions)}\")\n",
    "\n",
    "# Test inference timing\n",
    "print(\"\\n=== PERFORMANCE TIMING ===\")\n",
    "timing_tests = []\n",
    "for _ in range(100):\n",
    "    start_time = time.time()\n",
    "    pred, conf = pipeline.predict_met_class()\n",
    "    if pred is not None:\n",
    "        timing_tests.append((time.time() - start_time) * 1000)\n",
    "\n",
    "if timing_tests:\n",
    "    print(f\"Inference timing (100 tests):\")\n",
    "    print(f\"  - Mean: {np.mean(timing_tests):.2f}ms\")\n",
    "    print(f\"  - Std: {np.std(timing_tests):.2f}ms\") \n",
    "    print(f\"  - 95th percentile: {np.percentile(timing_tests, 95):.2f}ms\")\n",
    "    print(f\"  - Max: {np.max(timing_tests):.2f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753cfc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Export for Mobile Integration\n",
    "print(\"=== MODEL EXPORT FOR MOBILE ===\")\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create models directory\n",
    "models_dir = \"../models\"\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "os.makedirs(f\"{models_dir}/mobile\", exist_ok=True)\n",
    "\n",
    "# Save the complete model pipeline\n",
    "model_data = {\n",
    "    'model': best_model,\n",
    "    'scaler': scaler,\n",
    "    'feature_cols': feature_cols,\n",
    "    'window_size': best_window,\n",
    "    'model_name': best_model_name,\n",
    "    'accuracy': tuned_accuracy,\n",
    "    'parameters': best_params if 'best_params' in locals() else 'default'\n",
    "}\n",
    "\n",
    "# Save as pickle for Python usage\n",
    "model_file = f\"{models_dir}/met_model_final.pkl\"\n",
    "with open(model_file, 'wb') as f:\n",
    "    pickle.dump(model_data, f)\n",
    "\n",
    "print(f\"Model saved to: {model_file}\")\n",
    "\n",
    "# Save model metadata as JSON\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'window_size': best_window,\n",
    "    'feature_columns': feature_cols,\n",
    "    'accuracy': tuned_accuracy,\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'parameters': str(best_params) if 'best_params' in locals() else 'default',\n",
    "    'target_classes': ['Sedentary', 'Light', 'Moderate', 'Vigorous'],\n",
    "    'inference_time_ms': np.mean(timing_tests) if timing_tests else 'not_measured'\n",
    "}\n",
    "\n",
    "metadata_file = f\"{models_dir}/model_metadata.json\"\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"Metadata saved to: {metadata_file}\")\n",
    "\n",
    "# For iOS integration, save simplified model parameters\n",
    "if best_model_name == 'Random Forest':\n",
    "    # Extract tree structure for mobile implementation\n",
    "    mobile_params = {\n",
    "        'model_type': 'random_forest',\n",
    "        'n_estimators': best_model.n_estimators,\n",
    "        'max_depth': best_model.max_depth,\n",
    "        'feature_importances': best_model.feature_importances_.tolist(),\n",
    "        'feature_names': feature_cols\n",
    "    }\n",
    "elif best_model_name == 'SVM':\n",
    "    mobile_params = {\n",
    "        'model_type': 'svm',\n",
    "        'support_vectors': best_model.support_vectors_.tolist() if hasattr(best_model, 'support_vectors_') else [],\n",
    "        'dual_coef': best_model.dual_coef_.tolist() if hasattr(best_model, 'dual_coef_') else [],\n",
    "        'intercept': best_model.intercept_.tolist() if hasattr(best_model, 'intercept_') else []\n",
    "    }\n",
    "else:\n",
    "    mobile_params = {\n",
    "        'model_type': best_model_name.lower().replace(' ', '_'),\n",
    "        'note': 'Complex model - consider simplification for mobile'\n",
    "    }\n",
    "\n",
    "# Save scaler parameters for mobile\n",
    "scaler_params = {\n",
    "    'mean': scaler.mean_.tolist(),\n",
    "    'scale': scaler.scale_.tolist(),\n",
    "    'feature_names': feature_cols\n",
    "}\n",
    "\n",
    "mobile_model_file = f\"{models_dir}/mobile/model_params.json\"\n",
    "mobile_scaler_file = f\"{models_dir}/mobile/scaler_params.json\"\n",
    "\n",
    "with open(mobile_model_file, 'w') as f:\n",
    "    json.dump(mobile_params, f, indent=2)\n",
    "\n",
    "with open(mobile_scaler_file, 'w') as f:\n",
    "    json.dump(scaler_params, f, indent=2)\n",
    "\n",
    "print(f\"Mobile model parameters saved to: {mobile_model_file}\")\n",
    "print(f\"Mobile scaler parameters saved to: {mobile_scaler_file}\")\n",
    "\n",
    "# Create implementation recommendations\n",
    "recommendations = f\"\"\"\n",
    "=== IMPLEMENTATION RECOMMENDATIONS ===\n",
    "\n",
    "1. OPTIMAL CONFIGURATION:\n",
    "   - Model: {best_model_name}\n",
    "   - Window Size: {best_window} samples (~{best_window/20:.1f} seconds at 20Hz)\n",
    "   - Accuracy: {tuned_accuracy:.4f}\n",
    "   - Inference Time: {np.mean(timing_tests):.1f}ms (mobile suitable)\n",
    "\n",
    "2. TIME WINDOW INSIGHTS:\n",
    "   - Momentary acceleration spikes should NOT trigger immediate class changes\n",
    "   - Use {best_window} sample windows with 50% overlap for stability\n",
    "   - Apply smoothing filter to reduce noise from sudden movements\n",
    "   - Consider majority voting over 3 consecutive predictions\n",
    "\n",
    "3. MOBILE IMPLEMENTATION:\n",
    "   - Buffer {best_window} accelerometer readings\n",
    "   - Extract {len(feature_cols)} features per window\n",
    "   - Apply standardization using saved scaler parameters\n",
    "   - Predict using {best_model_name} model\n",
    "   - Use prediction smoothing to avoid rapid class switching\n",
    "\n",
    "4. FEATURE PRIORITIES:\n",
    "   Top 5 most important features for mobile optimization:\n",
    "\"\"\"\n",
    "\n",
    "if 'feature_importance_df' in locals():\n",
    "    for i, row in feature_importance_df.head(5).iterrows():\n",
    "        recommendations += f\"   - {row['feature']}: {row['importance']:.4f}\\n\"\n",
    "\n",
    "print(recommendations)\n",
    "\n",
    "# Save recommendations\n",
    "with open(f\"{models_dir}/implementation_recommendations.txt\", 'w') as f:\n",
    "    f.write(recommendations)\n",
    "\n",
    "print(f\"\\n=== MODEL SELECTION COMPLETE ===\")\n",
    "print(f\"Best model ready for iOS integration!\")\n",
    "print(f\"All files saved in {models_dir}/ directory\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
